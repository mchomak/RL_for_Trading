{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.15.0\n",
    "# !pip install gym==0.29.1\n",
    "# !pip install keras\n",
    "# !pip install keras-rl2\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipykernel\n",
    "# %pip install --upgrade nbformat\n",
    "# %pip install stable-baselines3[extra]\n",
    "# %pip install gymnasium==0.29.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stable-baselines3 plotly numpy pandas\n",
    "# %pip install ipywidgets\n",
    "# %pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "from model_config import Path\n",
    "import os\n",
    "import torch\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "decimal.getcontext().prec = 28  # Increase precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна. Работаем на GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Используем GPU\n",
    "    print(\"CUDA доступна. Работаем на GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Используем CPU\n",
    "    print(\"CUDA не доступна. Работаем на CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основано на Model 2, Отличия:\n",
    "- Меняем коэффициент в расчете reward \n",
    "\n",
    "**Если цена увеличилась** ($\\text{price\\_change} > 0$):\n",
    "\n",
    "  $$ \n",
    "  \\text{reward} += \\text{net\\_worth\\_change} - \\frac{\\left| \\left| \\text{net\\_worth\\_change} \\right| - \\left| \\text{price\\_change} \\right| \\right|}{1}\n",
    "  $$\n",
    "\n",
    "**Если цена уменьшилась** ($\\text{price\\_change} \\leq 0$):\n",
    "\n",
    "  $$\n",
    "  \\text{reward} += \\text{net\\_worth\\_change} + \\frac{\\left| \\left| \\text{net\\_worth\\_change} \\right| - \\left| \\text{price\\_change} \\right| \\right|}{1}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c615f0a610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_length = 48\n",
    "nb_steps = 150000\n",
    "\n",
    "model_num = 7\n",
    "data_num = 2\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed= seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset_file(dnum, folder=\"data\"):\n",
    "    pattern = f\"dataset_{dnum}D_*.csv\"\n",
    "    for filename in os.listdir(folder):\n",
    "        if fnmatch.fnmatch(filename, pattern):\n",
    "            return os.path.join(folder, filename)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\PythonScripts\\RL_for_Trading\\data\\processed\\dataset_2D_Standart_26cols_51tkn_1t.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(196939, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = find_dataset_file(data_num, folder= Path[\"processed\"])\n",
    "print(data_path)\n",
    "df = pd.read_csv(data_path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero 'Close' prices after scaling: 0\n"
     ]
    }
   ],
   "source": [
    "zero_close_prices = df[df['Close_orig'] == 0]\n",
    "print(f\"Number of zero 'Close' prices after scaling: {len(zero_close_prices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values = df['Asset_ID_encoded'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Asset_ID_encoded\n",
       "35    4340\n",
       "43    4340\n",
       "3     4339\n",
       "8     4339\n",
       "31    4336\n",
       "22    4336\n",
       "30    4333\n",
       "12    4331\n",
       "42    4327\n",
       "49    4325\n",
       "46    4324\n",
       "36    4323\n",
       "7     4319\n",
       "34    4317\n",
       "0     4317\n",
       "29    4311\n",
       "9     4309\n",
       "18    4298\n",
       "28    4287\n",
       "6     4267\n",
       "37    4265\n",
       "47    4254\n",
       "24    4253\n",
       "16    4251\n",
       "44    4238\n",
       "41    4170\n",
       "21    4165\n",
       "19    4154\n",
       "20    4085\n",
       "39    4038\n",
       "38    3987\n",
       "4     3900\n",
       "13    3899\n",
       "50    3888\n",
       "1     3888\n",
       "23    3886\n",
       "2     3846\n",
       "48    3698\n",
       "26    3685\n",
       "25    3678\n",
       "40    3622\n",
       "17    3622\n",
       "32    3504\n",
       "33    3216\n",
       "5     3024\n",
       "14    2909\n",
       "10    2907\n",
       "45    2906\n",
       "15    2263\n",
       "27    1943\n",
       "11     377\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = df['Asset_ID_encoded'].value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_asset_ids(df: pd.DataFrame, test_asset_ids: list):\n",
    "    test_df = df[df['Asset_ID_encoded'].isin(test_asset_ids)]\n",
    "    train_df = df[~df['Asset_ID_encoded'].isin(test_asset_ids)]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (158866, 26)\n",
      "Testing data shape: (38073, 26)\n"
     ]
    }
   ],
   "source": [
    "test_asset_ids = [45, 47, 32, 41, 7, 1, 38, 5, 31, 26]\n",
    "train_df, test_df = split_by_asset_ids(df = df, test_asset_ids = test_asset_ids)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Testing data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment for training an agent to trade on the exchange using a continuous action space.\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, mode):\n",
    "        super(TradingEnv, self).__init__()\n",
    "\n",
    "        # Save data and initialize parameters\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.total_steps = len(self.df) - 1\n",
    "        self.window_length = window_length\n",
    "        self.mode = mode # test or train\n",
    "\n",
    "        # Find indices where a new asset starts\n",
    "        self.asset_start_indices = self._find_asset_start_indices()\n",
    "        print(self.asset_start_indices)\n",
    "\n",
    "        # Define action space: Continuous action between -1 and 1\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Define observation space\n",
    "        num_features = len(self.df.columns) - 1 # Вычли Close_orig\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.window_length, num_features), dtype=np.float32)\n",
    "\n",
    "        # Initialize trading parameters\n",
    "        self.fee_cost = 0.001\n",
    "        self.initial_balance = 1000  # Starting balance\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.fut_net_worth = self.initial_balance\n",
    "\n",
    "        self.reward = 0\n",
    "        self.current_step = self.window_length\n",
    "        self.current_price = 0\n",
    "        self.tokens_held = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        self.now_token = (self.df.loc[self.current_step, 'Asset_ID_encoded'])\n",
    "        print(self.now_token)\n",
    "        self.prev_token = self.now_token\n",
    "\n",
    "        self.hist = {\n",
    "            \"current_step\": [],\n",
    "            'balance': [],\n",
    "            'net_worth': [],\n",
    "            'tokens_held': [],\n",
    "            \"token\": [],\n",
    "            \"current_price\": [],\n",
    "            \"reward\": [],\n",
    "            \"action\": [],\n",
    "            'total_shares_sold': [],\n",
    "            'total_sales_value': [],\n",
    "        }\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        if self.logger.hasHandlers():\n",
    "            self.logger.handlers.clear()\n",
    "\n",
    "        log_file = Path[\"train_log\"](model_num, data_num + 1)\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        self.logger.addHandler(file_handler)\n",
    "        logging.getLogger().handlers = []\n",
    "\n",
    "\n",
    "    def _find_asset_start_indices(self):\n",
    "        \"\"\"\n",
    "        Find the indices in the DataFrame where a new asset starts.\n",
    "        \"\"\"\n",
    "        asset_ids = self.df['Asset_ID_encoded']\n",
    "        start_indices = {asset_ids[0]:0}\n",
    "        for i in range(1, len(asset_ids)):\n",
    "            if asset_ids[i] != asset_ids[i - 1]:\n",
    "                start_indices[asset_ids[i]] = i\n",
    "\n",
    "        return start_indices\n",
    "\n",
    "\n",
    "    def reset(self, seed = seed, options=None, reset_hist=False):\n",
    "        super().reset(seed= seed)\n",
    "        self.logger.info(\"Environment reset\")\n",
    "\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.fut_net_worth = self.initial_balance\n",
    "        self.tokens_held = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        print(self.now_token)\n",
    "        keys = list(self.asset_start_indices.keys())\n",
    "        if len(keys) > (keys.index(self.now_token) + 1) and self.current_step != self.window_length:\n",
    "            self.now_token = keys[(keys.index(self.now_token) + 1)]\n",
    "            print(self.now_token, \"Change\")\n",
    "        else:\n",
    "            if self.mode == \"train\":\n",
    "                print(self.now_token, \"Restart\")\n",
    "                self.now_token = keys[0] \n",
    "            else:\n",
    "                return None, None\n",
    "\n",
    "        self.current_step = self.asset_start_indices[self.now_token] + self.window_length\n",
    "        self.now_token = self.df.loc[self.current_step, 'Asset_ID_encoded']\n",
    "        self.prev_token = self.now_token\n",
    "\n",
    "        self.logger.info(f\"Starting new episode with token {self.now_token} at step {self.current_step}\")\n",
    "\n",
    "        if reset_hist:\n",
    "            self.hist = {\n",
    "                \"current_step\": [],\n",
    "                'balance': [],\n",
    "                'net_worth': [],\n",
    "                'tokens_held': [],\n",
    "                \"token\": [],\n",
    "                \"current_price\": [],\n",
    "                \"reward\": [],\n",
    "                \"action\": [],\n",
    "                'total_shares_sold': [],\n",
    "                'total_sales_value': [],\n",
    "            }\n",
    "\n",
    "        observation = self._next_observation()\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def _next_observation(self):\n",
    "        frame = self.df.drop(columns = ['Close_orig']).loc[self.current_step - self.window_length + 1:self.current_step]\n",
    "        # obs = np.concatenate([\n",
    "        #     frame.values,\n",
    "        #     [self.tokens_held],\n",
    "        #     [self.balance],\n",
    "        #     [self.net_worth]\n",
    "        # ])\n",
    "        obs = frame.values\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        self.prev_token = self.now_token\n",
    "\n",
    "        if isinstance(action, (list, np.ndarray)):\n",
    "            action = action[0]\n",
    "\n",
    "        self.logger.info(f\"Step: {self.current_step}, Action taken: {action}\")\n",
    "        self._take_action(action)\n",
    "\n",
    "        self.current_step += 1  # Move to the next time step\n",
    "        \n",
    "        if self.current_step >= self.total_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            self.now_token = self.df.loc[self.current_step, 'Asset_ID_encoded']\n",
    "            if self.now_token != self.prev_token:\n",
    "                self.logger.info(f\"Token change at step {self.current_step}: {self.prev_token} -> {self.now_token}\")\n",
    "                terminated = True\n",
    "\n",
    "        if not terminated:\n",
    "            #  Теперь мы будем награждать модель используя данные за следующий шаг, а не предыдущий\n",
    "            self.future_price = self.df.loc[self.current_step, 'Close_orig'] \n",
    "            self.fut_net_worth = self.balance + self.tokens_held * self.future_price\n",
    "\n",
    "            if self.fut_net_worth != 0:\n",
    "                net_worth_change = (self.fut_net_worth * 100 / self.net_worth) - 100\n",
    "                price_change = (self.future_price * 100 / self.current_price) - 100\n",
    "                initial_change = self.net_worth / self.initial_balance\n",
    "                \n",
    "                if price_change > 0:\n",
    "                    self.reward += net_worth_change - (abs(abs(net_worth_change) - abs(price_change)) / 1)\n",
    "                else:\n",
    "                    self.reward += net_worth_change + (abs(abs(net_worth_change) - abs(price_change)) / 1)\n",
    "\n",
    "                if self.net_worth > self.initial_balance:\n",
    "                    self.reward += initial_change\n",
    "                else:\n",
    "                    self.reward += -(1 - initial_change)\n",
    "                \n",
    "                self.logger.info(f\"step: {self.current_price - 1}, net_worth_change: {net_worth_change}, price_change: {price_change}, initial_change: {initial_change}, reward: {self.reward}\")\n",
    "\n",
    "                if self.net_worth < self.initial_balance * 0.5: # Только во время тренировки штрафуем за проеб половины баланса\n",
    "                    if self.mode == \"train\":\n",
    "                        self.logger.info(\"Net worth dropped below 50% of initial balance.\")\n",
    "                        terminated = True\n",
    "            else:\n",
    "                self.logger.info(\"fut_net_worth == 0\")\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        info = {}\n",
    "\n",
    "        self.logger.info(f\"Net worth: {self.net_worth}, Balance: {self.balance}, Reward: {self.reward}\")\n",
    "\n",
    "        self.hist[\"current_step\"].append(self.current_step - 1)\n",
    "        self.hist[\"balance\"].append(self.balance)\n",
    "        self.hist[\"net_worth\"].append(self.net_worth)\n",
    "        self.hist[\"tokens_held\"].append(self.tokens_held)\n",
    "        self.hist[\"token\"].append(self.now_token)\n",
    "        self.hist[\"current_price\"].append(self.current_price)\n",
    "        self.hist[\"reward\"].append(self.reward)\n",
    "        self.hist[\"action\"].append(action)\n",
    "        self.hist[\"total_shares_sold\"].append(self.total_shares_sold)\n",
    "        self.hist[\"total_sales_value\"].append(self.total_sales_value)\n",
    "\n",
    "        return obs, self.reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \"\"\"\n",
    "        Apply the continuous action to the current state.\n",
    "        \"\"\"\n",
    "        self.current_price = self.df.loc[self.current_step, 'Close_orig']\n",
    "\n",
    "        action = float(np.clip(action, -1, 1))\n",
    "\n",
    "        if action < 0:\n",
    "            proportion = -action  # Convert to positive\n",
    "            shares_to_sell = int(self.tokens_held * proportion)\n",
    "            self._sell(shares_to_sell)\n",
    "\n",
    "        elif action > 0:\n",
    "            proportion = action\n",
    "            self._buy(proportion)\n",
    "        else:\n",
    "            self.reward += -1\n",
    "            pass  \n",
    "\n",
    "        self.net_worth = self.balance + self.tokens_held * self.current_price\n",
    "\n",
    "\n",
    "    def _buy(self, proportion):\n",
    "        amount_to_spend = self.balance * proportion\n",
    "\n",
    "        shares_to_buy = int(amount_to_spend / (self.current_price * (1 + self.fee_cost)))\n",
    "\n",
    "        if shares_to_buy > 0:\n",
    "            total_cost = shares_to_buy * self.current_price\n",
    "            transaction_cost = total_cost * self.fee_cost\n",
    "            total_cost += transaction_cost\n",
    "\n",
    "            self.balance -= total_cost\n",
    "            self.tokens_held += shares_to_buy\n",
    "\n",
    "            self.logger.info(f\"Bought {shares_to_buy} shares at price {self.current_price}\")\n",
    "            self.logger.info(f\"Total cost: {total_cost}, Transaction cost: {transaction_cost}\")\n",
    "        else:\n",
    "            self.reward += -5\n",
    "            self.logger.info(\"Not enough balance to buy.\")\n",
    "\n",
    "\n",
    "    def _sell(self, shares_to_sell):\n",
    "        if shares_to_sell > self.tokens_held:\n",
    "            shares_to_sell = self.tokens_held  # Can't sell more than held\n",
    "\n",
    "        if shares_to_sell > 0:\n",
    "            total_sale = shares_to_sell * self.current_price\n",
    "            transaction_cost = total_sale * self.fee_cost\n",
    "            total_sale -= transaction_cost\n",
    "\n",
    "            self.balance += total_sale\n",
    "            self.tokens_held -= shares_to_sell\n",
    "            self.total_shares_sold += shares_to_sell\n",
    "            self.total_sales_value += total_sale\n",
    "\n",
    "            self.logger.info(f\"Sold {shares_to_sell} shares at price {self.current_price}\")\n",
    "            self.logger.info(f\"Total sale: {total_sale}, Transaction cost: {transaction_cost}\")\n",
    "        else:\n",
    "            self.reward += -5\n",
    "            self.logger.info(\"No shares to sell.\")\n",
    "\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        profit = self.net_worth - self.initial_balance\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance:.2f}')\n",
    "        print(f'Shares held: {self.tokens_held}')\n",
    "        print(f'Net worth: {self.net_worth:.2f}')\n",
    "        print(f'Profit: {profit:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 2: 4317, 3: 8163, 4: 12502, 6: 16402, 8: 20669, 9: 25008, 10: 29317, 11: 32224, 12: 32601, 13: 36932, 14: 40831, 15: 43740, 16: 46003, 17: 50254, 18: 53876, 19: 58174, 20: 62328, 21: 66413, 22: 70578, 23: 74914, 24: 78800, 25: 83053, 27: 86731, 28: 88674, 29: 92961, 30: 97272, 33: 101605, 34: 104821, 35: 109138, 36: 113478, 37: 117801, 39: 122066, 40: 126104, 42: 129726, 43: 134053, 44: 138393, 46: 142631, 48: 146955, 49: 150653, 50: 154978}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_env = TradingEnv(train_df, mode = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 5: 3888, 7: 6912, 26: 11231, 31: 14916, 32: 19252, 38: 22756, 41: 26743, 45: 30913, 47: 33819}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test_env = TradingEnv(test_df, mode = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_env(train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisaton Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_close_by_asset(df, asset_id):\n",
    "    # Фильтрация данных по Asset_ID_encoded\n",
    "    asset_data = df[df['Asset_ID_encoded'] == asset_id]\n",
    "    \n",
    "    # Построение графика Close к индексу DataFrame\n",
    "    fig = px.line(asset_data, x=asset_data.index, y='Close_orig', \n",
    "                  title=f'Close Price for Asset ID {asset_id}', \n",
    "                  labels={'index': 'Index', 'Close': 'Close Price'})\n",
    "    \n",
    "    # Показать график\n",
    "    fig.show()\n",
    "    fig.write_html(Path[\"plots\"](model_num, data_num, \"close_by_asset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_data(df, token):\n",
    "    # Фильтрация данных по выбранному токену\n",
    "    token_data = df[df['token'] == token]\n",
    "    \n",
    "    # Вычисление среднего значения net_worth для данного токена\n",
    "    avg_net_worth = token_data['reward'].mean()\n",
    "\n",
    "    # Создание графика\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Линия net_worth\n",
    "    fig.add_trace(go.Scatter(x=token_data['current_step'], y=token_data['reward'], mode='lines', name='Reward'))\n",
    "\n",
    "    # Горизонтальная линия для среднего значения net_worth\n",
    "    fig.add_hline(y=avg_net_worth, line_color=\"red\", name=f'Average Reward = {avg_net_worth:.2f}')\n",
    "\n",
    "    # Настройка заголовков и осей\n",
    "    fig.update_layout(title=f'Reward and Average for {token}',\n",
    "                      xaxis_title='Current Step',\n",
    "                      yaxis_title='Reward')\n",
    "\n",
    "    # Показать график\n",
    "    fig.show()\n",
    "    fig.write_html(Path[\"plots\"](model_num, data_num, \"reward_by_asset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_change_by_asset(df, asset_id):\n",
    "    # Фильтрация данных по Asset_ID_encoded\n",
    "    asset_data = df[df['Asset_ID_encoded'] == asset_id].copy()\n",
    "    \n",
    "    # Вычисление процентного изменения цены (Close)\n",
    "    asset_data['Price_Change_Percent'] = asset_data['Close_orig'].pct_change() * 100\n",
    "    \n",
    "    # Построение графика изменения цены в процентах\n",
    "    fig = px.line(asset_data, x=asset_data.index, y='Price_Change_Percent', \n",
    "                  title=f'Price Change Percentage for Asset ID {asset_id}', \n",
    "                  labels={'index': 'Index', 'Price_Change_Percent': 'Price Change (%)'})\n",
    "    \n",
    "    # Показать график\n",
    "    fig.show()\n",
    "    fig.write_html(Path[\"plots\"](model_num, data_num, \"price_change_by_asset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_data(df, token):\n",
    "    # Фильтрация данных по выбранному токену\n",
    "    token_data = df[df['token'] == token]\n",
    "    \n",
    "    # Вычисление среднего значения net_worth для данного токена\n",
    "    avg_net_worth = token_data['net_worth'].mean()\n",
    "\n",
    "    # Создание графика\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Линия net_worth\n",
    "    fig.add_trace(go.Scatter(x=token_data['current_step'], y=token_data['net_worth'], mode='lines', name='Net Worth'))\n",
    "\n",
    "    # Горизонтальная линия для net_worth = 1000\n",
    "    fig.add_hline(y=1000, line_color=\"green\", name='Net Worth = 1000')\n",
    "\n",
    "    # Горизонтальная линия для среднего значения net_worth\n",
    "    fig.add_hline(y=avg_net_worth, line_color=\"red\", name=f'Average Net Worth = {avg_net_worth:.2f}')\n",
    "\n",
    "    # Настройка заголовков и осей\n",
    "    fig.update_layout(title=f'Net Worth and Average for {token}',\n",
    "                      xaxis_title='Current Step',\n",
    "                      yaxis_title='Net Worth')\n",
    "\n",
    "    # Показать график\n",
    "    fig.show()\n",
    "    fig.write_html(Path[\"plots\"](model_num, data_num, \"token_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_action_counts(df, token):\n",
    "    # Фильтрация данных по токену\n",
    "    token_data = df[df['token'] == token]\n",
    "    \n",
    "    # Подсчет количества каждого уникального действия для данного токена\n",
    "    action_counts = token_data['action'].value_counts().reset_index()\n",
    "    action_counts.columns = ['action', 'count']\n",
    "\n",
    "    # Построение бар-чарта для отображения количества каждого действия\n",
    "    fig = px.bar(action_counts, x='action', y='count', title=f'Count of Actions for {token}', labels={'action': 'Action', 'count': 'Count'})\n",
    "\n",
    "    # Показать график\n",
    "    fig.show()\n",
    "    fig.write_html(Path[\"plots\"](model_num, data_num, \"action_counts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_change_by_token(df, token):\n",
    "    # Фильтрация данных по токену\n",
    "    token_data = df[df['token'] == token].copy()\n",
    "\n",
    "    # Вычисление относительного изменения для current_price и net_worth\n",
    "    token_data['Price_Change_Percent'] = token_data['current_price'].pct_change() * 100\n",
    "    token_data['NetWorth_Change_Percent'] = token_data['net_worth'].pct_change() * 100\n",
    "\n",
    "    # Создание графика\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Линия для изменения current_price\n",
    "    fig.add_trace(go.Scatter(x=token_data['current_step'], y=token_data['Price_Change_Percent'],\n",
    "                             mode='lines', name='Current Price Change (%)'))\n",
    "\n",
    "    # Линия для изменения net_worth\n",
    "    fig.add_trace(go.Scatter(x=token_data['current_step'], y=token_data['NetWorth_Change_Percent'],\n",
    "                             mode='lines', name='Net Worth Change (%)'))\n",
    "\n",
    "    # Настройка заголовков и осей\n",
    "    fig.update_layout(title=f'Relative Change of Current Price and Net Worth for {token}',\n",
    "                      xaxis_title='Current Step',\n",
    "                      yaxis_title='Change (%)')\n",
    "\n",
    "    # Показать график\n",
    "    fig.show()\n",
    "    fig.write_html(Path[\"plots\"](model_num, data_num, \"relative_change_by_token\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\n",
    "    policy='MlpPolicy',  # Use a Multi-Layer Perceptron policy\n",
    "    env=train_env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-4,  # Adjust learning rate if needed\n",
    "    batch_size=128,      # Adjust batch size if needed\n",
    "    tensorboard_log=\"./sac_tensorboard/\",  # Directory for TensorBoard logs\n",
    "    seed= seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluation callback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "eval_callback = EvalCallback(\n",
    "    test_env,                         # Evaluation environment\n",
    "    best_model_save_path= Path[\"model_save\"](model_num, data_num),   # Directory to save the best model\n",
    "    log_path= Path[\"train_log\"](model_num, data_num),               # Directory to save evaluation logs\n",
    "    eval_freq=5000,                   # Evaluate every 5000 steps\n",
    "    n_eval_episodes=3,                # Number of episodes to evaluate\n",
    "    deterministic=True,               # Use deterministic actions during evaluation\n",
    "    render=False                      # Disable rendering during evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2 Change\n",
      "Logging to ./sac_tensorboard/SAC_33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b954ba89374a4677b8dd9cf49762dd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callback=eval_callback   \u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[0;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:557\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mreset_noise(env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# Select action randomly or according to policy\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m    560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:390\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._sample_action\u001b[1;34m(self, learning_starts, action_noise, n_envs)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;66;03m# Note: when using continuous actions,\u001b[39;00m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;66;03m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself._last_obs was not set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 390\u001b[0m     unscaled_action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:365\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[1;32m--> 365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(obs_tensor, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:276\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mobs_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs_tensor, vectorized_env\n",
      "File \u001b[1;32md:\\PythonScripts\\RL_for_Trading\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:485\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[1;34m(obs, device)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03mMoves the observation to the given device.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m:return: PyTorch tensor of the observation on a desired device.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: th\u001b[38;5;241m.\u001b[39mas_tensor(_obs, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(\n",
    "    total_timesteps=nb_steps,\n",
    "    log_interval=1000,        \n",
    "    # callback=eval_callback   \n",
    "    progress_bar= True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(Path[\"model_save\"](model_num, data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = train_env.hist\n",
    "train_hist_df = pd.DataFrame(train_hist)\n",
    "print(len(train_hist[\"action\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = train_hist_df['token'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 25\n",
    "plot_token_data(df = train_hist_df, token = token)\n",
    "plot_close_by_asset(df= train_df, asset_id= token)\n",
    "plot_price_change_by_asset(df= train_df, asset_id= token)\n",
    "plot_relative_change_by_token(df = train_hist_df, token = token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load(Path[\"model_save\"](model_num, data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = test_env.reset(reset_hist=True)  # Reset hist at the beginning\n",
    "for _ in range(len(test_df)):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = test_env.reset(reset_hist=False)  # Do not reset hist\n",
    "        if info is None:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hist = test_env.hist\n",
    "test_hist_df = pd.DataFrame(test_hist)\n",
    "print(len(test_hist[\"action\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = test_hist_df['token'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 22\n",
    "plot_token_data(df = test_hist_df, token = token)\n",
    "plot_close_by_asset(df= test_df, asset_id= token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(hist_df, test_df, initial_balance):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance.\n",
    "\n",
    "    Parameters:\n",
    "    - hist_df: DataFrame containing the testing history.\n",
    "    - test_df: DataFrame containing the test data.\n",
    "    - initial_balance: Initial balance used in the environment.\n",
    "\n",
    "    Returns:\n",
    "    - report_df: DataFrame containing performance metrics per asset.\n",
    "    - overall_metrics: Dictionary containing overall performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure timestamps are in order\n",
    "    hist_df = hist_df.sort_values('current_step').reset_index(drop=True)\n",
    "\n",
    "    # List of assets\n",
    "    assets = hist_df['token'].unique()\n",
    "\n",
    "    # Initialize report DataFrame\n",
    "    report = []\n",
    "\n",
    "    for asset_id in assets:\n",
    "        asset_hist = hist_df[hist_df['token'] == asset_id]\n",
    "        asset_data = test_df[test_df['Asset_ID_encoded'] == asset_id]\n",
    "\n",
    "        # Calculate total profit/loss\n",
    "        final_net_worth = asset_hist['net_worth'].iloc[-1]\n",
    "        total_profit = final_net_worth - initial_balance\n",
    "\n",
    "        # Calculate ROI\n",
    "        roi = (final_net_worth - initial_balance) / initial_balance * 100\n",
    "\n",
    "        # Calculate Sharpe Ratio\n",
    "        returns = asset_hist['net_worth'].pct_change().dropna()\n",
    "        if returns.std() != 0:\n",
    "            sharpe_ratio = (returns.mean() / returns.std()) * np.sqrt(252)  # Assuming daily data\n",
    "        else:\n",
    "            sharpe_ratio = np.nan  # Undefined if no variance\n",
    "\n",
    "        # Calculate Maximum Drawdown\n",
    "        cumulative_returns = (1 + returns).cumprod()\n",
    "        cumulative_max = cumulative_returns.cummax()\n",
    "        drawdown = (cumulative_returns - cumulative_max) / cumulative_max\n",
    "        max_drawdown = drawdown.min()\n",
    "\n",
    "        # Calculate Win Rate\n",
    "        trades = asset_hist[asset_hist['action'] != 0]\n",
    "        wins = trades[trades['net_worth'].diff() > 0]\n",
    "        win_rate = len(wins) / len(trades) * 100 if len(trades) > 0 else np.nan\n",
    "\n",
    "        # Buy-and-Hold Strategy\n",
    "        initial_price = asset_data['Close_orig'].iloc[0]\n",
    "        final_price = asset_data['Close_orig'].iloc[-1]\n",
    "        buy_and_hold_profit = (final_price - initial_price) * (initial_balance / initial_price)\n",
    "        buy_and_hold_roi = (final_price - initial_price) / initial_price * 100\n",
    "\n",
    "        # Ideal Strategy\n",
    "        min_price = asset_data['Close_orig'].min()\n",
    "        max_price = asset_data['Close_orig'].max()\n",
    "        ideal_profit = (max_price - min_price) * (initial_balance / min_price)\n",
    "        ideal_roi = (max_price - min_price) / min_price * 100\n",
    "\n",
    "        # Collect metrics\n",
    "        report.append({\n",
    "            'Asset_ID': asset_id,\n",
    "            'Total Profit': total_profit,\n",
    "            'ROI (%)': roi,\n",
    "            'Sharpe Ratio': sharpe_ratio,\n",
    "            'Max Drawdown (%)': max_drawdown * 100,\n",
    "            'Win Rate (%)': win_rate,\n",
    "            'Buy-and-Hold Profit': buy_and_hold_profit,\n",
    "            'Buy-and-Hold ROI (%)': buy_and_hold_roi,\n",
    "            'Ideal Profit': ideal_profit,\n",
    "            'Ideal ROI (%)': ideal_roi,\n",
    "            'Asset Price Change (%)': (final_price - initial_price) / initial_price * 100,\n",
    "        })\n",
    "\n",
    "    # Create DataFrame from report\n",
    "    report_df = pd.DataFrame(report)\n",
    "\n",
    "    # Calculate overall averages for each column\n",
    "    averages = report_df.mean(numeric_only=True)\n",
    "    averages['Asset_ID'] = 'Average'  # Mark row as average\n",
    "\n",
    "    # Append averages row to the DataFrame using pd.concat\n",
    "    report_df = pd.concat([report_df, pd.DataFrame([averages])], ignore_index=True)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_profit = report_df['Total Profit'].sum()\n",
    "    overall_roi = (overall_profit / (initial_balance * len(assets))) * 100\n",
    "    overall_sharpe = report_df['Sharpe Ratio'].mean()\n",
    "    overall_win_rate = report_df['Win Rate (%)'].mean()\n",
    "    overall_buy_and_hold_profit = report_df['Buy-and-Hold Profit'].sum()\n",
    "    overall_buy_and_hold_roi = (overall_buy_and_hold_profit / (initial_balance * len(assets))) * 100\n",
    "\n",
    "    overall_metrics = {\n",
    "        'Total Profit': overall_profit,\n",
    "        'ROI (%)': overall_roi,\n",
    "        'Sharpe Ratio': overall_sharpe,\n",
    "        'Win Rate (%)': overall_win_rate,\n",
    "        'Buy-and-Hold Profit': overall_buy_and_hold_profit,\n",
    "        'Buy-and-Hold ROI (%)': overall_buy_and_hold_roi,\n",
    "    }\n",
    "\n",
    "    return report_df, overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Asset_ID: Уникальный идентификатор актива (из столбца token), для которого рассчитываются метрики.\n",
    "\n",
    "- Total Profit: Общий финансовый результат (прибыль или убыток) по данному активу. Рассчитывается как разница между конечной чистой стоимостью (net_worth) и начальным балансом (initial_balance).\n",
    "\n",
    "- ROI (%): Доходность инвестиций (Return on Investment) в процентах. Показывает процентный прирост (или убыток) от начальной суммы баланса.\n",
    "\n",
    "- Sharpe Ratio: Коэффициент Шарпа. Оценивает отношение доходности к риску (волатильности). Чем выше коэффициент Шарпа, тем лучше риск-корректированная доходность стратегии.\n",
    "\n",
    "- Max Drawdown (%): Максимальная просадка в процентах. Это максимальное снижение стоимости актива от его исторического максимума. Отражает риски стратегии, связанные с падением стоимости.\n",
    "\n",
    "- Win Rate (%): Процент прибыльных сделок. Это отношение количества прибыльных сделок к общему количеству сделок по активу, умноженное на 100.\n",
    "\n",
    "- Buy-and-Hold Profit: Прибыль при стратегии \"купить и держать\". Показывает, сколько можно было бы заработать, если просто купить актив в начале и держать его до конца периода тестирования.\n",
    "\n",
    "- Buy-and-Hold ROI (%): Доходность при стратегии \"купить и держать\". Процентный прирост от начальной цены актива, если его просто держать до конца периода.\n",
    "\n",
    "- Ideal Profit: Идеальная прибыль. Это гипотетическая максимальная прибыль, которую можно было бы получить, если бы купили актив по минимальной цене и продали по максимальной цене за период.\n",
    "\n",
    "- Ideal ROI (%): Идеальная доходность. Процентный прирост при идеальной стратегии, где покупка происходит по минимальной цене, а продажа — по максимальной.\n",
    "\n",
    "- Asset Price Change (%): Изменение цены актива в процентах за период. Это процентное изменение цены от начальной до конечной за период тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df, overall_metrics = evaluate_model(test_hist_df, test_df, initial_balance = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hist_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_csv(Path[\"reports\"](model_num, data_num, \"test\"), index= False)\n",
    "test_hist_df.describe().to_csv(Path[\"reports\"](model_num, data_num, \"test_describe\"), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
